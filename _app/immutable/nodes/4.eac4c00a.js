import{S as pe,i as me,s as ge,k as n,q as h,a as b,l as s,m as i,r as d,h as r,c as _,n as t,b as ve,F as e,J as Y}from"../chunks/index.6a9e29a0.js";function be(H){let a,y,k,E,I,M,D,w,W,X,A,B,J,o,T,f,j,z,P,u,K,O,x,p,C,F,L,m,U,G,N,g,Q,R,q,v,V;return{c(){a=n("main"),y=n("p"),k=h("WIP: barebones page to important links to this work"),E=b(),I=n("h1"),M=h("Energy Transformer"),D=b(),w=n("p"),W=h(`Our work combines aspects of three promising paradigms in machine learning,
    namely, attention mechanism, energy-based models, and associative memory.
    Attention is the power-house driving modern deep learning successes, but it
    lacks clear theoretical foundations. Energy-based models allow a principled
    approach to discriminative and generative tasks, but the design of the
    energy functional is not straightforward. At the same time, Dense
    Associative Memory models or Modern Hopfield Networks have a
    well-established theoretical foundation, and allow an intuitive design of
    the energy function. We propose a novel architecture, called the Energy
    Transformer (or ET for short), that uses a sequence of attention layers that
    are purposely designed to minimize a specifically engineered energy
    function, which is responsible for representing the relationships between
    the tokens. In this work, we introduce the theoretical foundations of ET,
    explore its empirical capabilities using the image completion task, and
    obtain strong quantitative results on the graph anomaly detection and graph
    classification tasks.`),X=b(),A=n("h2"),B=h("Links"),J=b(),o=n("ul"),T=n("li"),f=n("a"),j=h("Paper on ArXiv"),z=b(),P=n("li"),u=n("a"),K=h("NeurIPS 2023 virtual page"),O=b(),x=n("li"),p=n("a"),C=h("JAX code for Energy Transformer"),F=b(),L=n("li"),m=n("a"),U=h("Ben presents for NeurIPS 2023 (5 min)"),G=b(),N=n("li"),g=n("a"),Q=h("Initial Twitter Announcement"),R=b(),q=n("li"),v=n("a"),V=h("Dima presents at KITP 2023 (45 min)"),this.h()},l(S){a=s(S,"MAIN",{});var l=i(a);y=s(l,"P",{class:!0});var Z=i(y);k=d(Z,"WIP: barebones page to important links to this work"),Z.forEach(r),E=_(l),I=s(l,"H1",{});var $=i(I);M=d($,"Energy Transformer"),$.forEach(r),D=_(l),w=s(l,"P",{class:!0});var ee=i(w);W=d(ee,`Our work combines aspects of three promising paradigms in machine learning,
    namely, attention mechanism, energy-based models, and associative memory.
    Attention is the power-house driving modern deep learning successes, but it
    lacks clear theoretical foundations. Energy-based models allow a principled
    approach to discriminative and generative tasks, but the design of the
    energy functional is not straightforward. At the same time, Dense
    Associative Memory models or Modern Hopfield Networks have a
    well-established theoretical foundation, and allow an intuitive design of
    the energy function. We propose a novel architecture, called the Energy
    Transformer (or ET for short), that uses a sequence of attention layers that
    are purposely designed to minimize a specifically engineered energy
    function, which is responsible for representing the relationships between
    the tokens. In this work, we introduce the theoretical foundations of ET,
    explore its empirical capabilities using the image completion task, and
    obtain strong quantitative results on the graph anomaly detection and graph
    classification tasks.`),ee.forEach(r),X=_(l),A=s(l,"H2",{class:!0});var te=i(A);B=d(te,"Links"),te.forEach(r),J=_(l),o=s(l,"UL",{});var c=i(o);T=s(c,"LI",{});var re=i(T);f=s(re,"A",{href:!0,target:!0,rel:!0,class:!0});var ae=i(f);j=d(ae,"Paper on ArXiv"),ae.forEach(r),re.forEach(r),z=_(c),P=s(c,"LI",{});var ne=i(P);u=s(ne,"A",{href:!0,target:!0,rel:!0,class:!0});var se=i(u);K=d(se,"NeurIPS 2023 virtual page"),se.forEach(r),ne.forEach(r),O=_(c),x=s(c,"LI",{});var ie=i(x);p=s(ie,"A",{href:!0,target:!0,rel:!0,class:!0});var oe=i(p);C=d(oe,"JAX code for Energy Transformer"),oe.forEach(r),ie.forEach(r),F=_(c),L=s(c,"LI",{});var le=i(L);m=s(le,"A",{href:!0,target:!0,rel:!0,class:!0});var ce=i(m);U=d(ce,"Ben presents for NeurIPS 2023 (5 min)"),ce.forEach(r),le.forEach(r),G=_(c),N=s(c,"LI",{});var he=i(N);g=s(he,"A",{href:!0,target:!0,rel:!0,class:!0});var de=i(g);Q=d(de,"Initial Twitter Announcement"),de.forEach(r),he.forEach(r),R=_(c),q=s(c,"LI",{});var fe=i(q);v=s(fe,"A",{href:!0,target:!0,rel:!0,class:!0});var ue=i(v);V=d(ue,"Dima presents at KITP 2023 (45 min)"),ue.forEach(r),fe.forEach(r),c.forEach(r),l.forEach(r),this.h()},h(){t(y,"class","text-gray-500 text-sm italic my-8"),t(w,"class","abstract"),t(A,"class","svelte-1ll056c"),t(f,"href","https://arxiv.org/abs/2302.07253"),t(f,"target","_blank"),t(f,"rel","noreferrer"),t(f,"class","svelte-1ll056c"),t(u,"href","https://neurips.cc/virtual/2023/poster/71901"),t(u,"target","_blank"),t(u,"rel","noreferrer"),t(u,"class","svelte-1ll056c"),t(p,"href","https://github.com/bhoov/energy-transformer-jax"),t(p,"target","_blank"),t(p,"rel","noreferrer"),t(p,"class","svelte-1ll056c"),t(m,"href","https://recorder-v3.slideslive.com/#/share?share=88188&s=6a61b5bf-c59b-4ef5-b71f-be4156c92dd5"),t(m,"target","_blank"),t(m,"rel","noreferrer"),t(m,"class","svelte-1ll056c"),t(g,"href","https://twitter.com/Ben_Hoov/status/1730570603210404236"),t(g,"target","_blank"),t(g,"rel","noreferrer"),t(g,"class","svelte-1ll056c"),t(v,"href","https://online.kitp.ucsb.edu/online/deeplearning23/krotov/rm/jwvideo.html"),t(v,"target","_blank"),t(v,"rel","noreferrer"),t(v,"class","svelte-1ll056c")},m(S,l){ve(S,a,l),e(a,y),e(y,k),e(a,E),e(a,I),e(I,M),e(a,D),e(a,w),e(w,W),e(a,X),e(a,A),e(A,B),e(a,J),e(a,o),e(o,T),e(T,f),e(f,j),e(o,z),e(o,P),e(P,u),e(u,K),e(o,O),e(o,x),e(x,p),e(p,C),e(o,F),e(o,L),e(L,m),e(m,U),e(o,G),e(o,N),e(N,g),e(g,Q),e(o,R),e(o,q),e(q,v),e(v,V)},p:Y,i:Y,o:Y,d(S){S&&r(a)}}}function _e(H,a,y){let{data:k}=a;return H.$$set=E=>{"data"in E&&y(0,k=E.data)},[k]}class ke extends pe{constructor(a){super(),me(this,a,_e,be,ge,{data:0})}}export{ke as component};
