import{S as _e,i as Ee,s as ke,k as a,q as h,a as f,l as n,m as s,r as d,h as r,c as p,n as t,b as we,F as e,J as te}from"../chunks/index.6a9e29a0.js";function Ae(M){let o,E,k,w,T,D,W,A,X,B,I,J,z,i,P,u,K,O,x,m,C,F,L,g,U,G,j,v,Q,R,N,b,V,Y,q,y,Z,$,S,_,ee;return{c(){o=a("main"),E=a("p"),k=h("WIP: barebones page to important links to this work"),w=f(),T=a("h1"),D=h("Energy Transformer"),W=f(),A=a("p"),X=h(`Our work combines aspects of three promising paradigms in machine learning,
    namely, attention mechanism, energy-based models, and associative memory.
    Attention is the power-house driving modern deep learning successes, but it
    lacks clear theoretical foundations. Energy-based models allow a principled
    approach to discriminative and generative tasks, but the design of the
    energy functional is not straightforward. At the same time, Dense
    Associative Memory models or Modern Hopfield Networks have a
    well-established theoretical foundation, and allow an intuitive design of
    the energy function. We propose a novel architecture, called the Energy
    Transformer (or ET for short), that uses a sequence of attention layers that
    are purposely designed to minimize a specifically engineered energy
    function, which is responsible for representing the relationships between
    the tokens. In this work, we introduce the theoretical foundations of ET,
    explore its empirical capabilities using the image completion task, and
    obtain strong quantitative results on the graph anomaly detection and graph
    classification tasks.`),B=f(),I=a("h2"),J=h("Links"),z=f(),i=a("ul"),P=a("li"),u=a("a"),K=h("Paper on ArXiv"),O=f(),x=a("li"),m=a("a"),C=h("NeurIPS 2023 virtual page"),F=f(),L=a("li"),g=a("a"),U=h("JAX code for Energy Transformer"),G=f(),j=a("li"),v=a("a"),Q=h("PyTorch code for Energy Transformer"),R=f(),N=a("li"),b=a("a"),V=h("Ben presents for NeurIPS 2023 (5 min)"),Y=f(),q=a("li"),y=a("a"),Z=h("Initial Twitter Announcement"),$=f(),S=a("li"),_=a("a"),ee=h("Dima presents at KITP 2023 (45 min)"),this.h()},l(H){o=n(H,"MAIN",{});var c=s(o);E=n(c,"P",{class:!0});var re=s(E);k=d(re,"WIP: barebones page to important links to this work"),re.forEach(r),w=p(c),T=n(c,"H1",{});var ae=s(T);D=d(ae,"Energy Transformer"),ae.forEach(r),W=p(c),A=n(c,"P",{class:!0});var ne=s(A);X=d(ne,`Our work combines aspects of three promising paradigms in machine learning,
    namely, attention mechanism, energy-based models, and associative memory.
    Attention is the power-house driving modern deep learning successes, but it
    lacks clear theoretical foundations. Energy-based models allow a principled
    approach to discriminative and generative tasks, but the design of the
    energy functional is not straightforward. At the same time, Dense
    Associative Memory models or Modern Hopfield Networks have a
    well-established theoretical foundation, and allow an intuitive design of
    the energy function. We propose a novel architecture, called the Energy
    Transformer (or ET for short), that uses a sequence of attention layers that
    are purposely designed to minimize a specifically engineered energy
    function, which is responsible for representing the relationships between
    the tokens. In this work, we introduce the theoretical foundations of ET,
    explore its empirical capabilities using the image completion task, and
    obtain strong quantitative results on the graph anomaly detection and graph
    classification tasks.`),ne.forEach(r),B=p(c),I=n(c,"H2",{class:!0});var se=s(I);J=d(se,"Links"),se.forEach(r),z=p(c),i=n(c,"UL",{});var l=s(i);P=n(l,"LI",{});var oe=s(P);u=n(oe,"A",{href:!0,target:!0,rel:!0,class:!0});var ie=s(u);K=d(ie,"Paper on ArXiv"),ie.forEach(r),oe.forEach(r),O=p(l),x=n(l,"LI",{});var le=s(x);m=n(le,"A",{href:!0,target:!0,rel:!0,class:!0});var ce=s(m);C=d(ce,"NeurIPS 2023 virtual page"),ce.forEach(r),le.forEach(r),F=p(l),L=n(l,"LI",{});var he=s(L);g=n(he,"A",{href:!0,target:!0,rel:!0,class:!0});var de=s(g);U=d(de,"JAX code for Energy Transformer"),de.forEach(r),he.forEach(r),G=p(l),j=n(l,"LI",{});var fe=s(j);v=n(fe,"A",{href:!0,target:!0,rel:!0,class:!0});var pe=s(v);Q=d(pe,"PyTorch code for Energy Transformer"),pe.forEach(r),fe.forEach(r),R=p(l),N=n(l,"LI",{});var ue=s(N);b=n(ue,"A",{href:!0,target:!0,rel:!0,class:!0});var me=s(b);V=d(me,"Ben presents for NeurIPS 2023 (5 min)"),me.forEach(r),ue.forEach(r),Y=p(l),q=n(l,"LI",{});var ge=s(q);y=n(ge,"A",{href:!0,target:!0,rel:!0,class:!0});var ve=s(y);Z=d(ve,"Initial Twitter Announcement"),ve.forEach(r),ge.forEach(r),$=p(l),S=n(l,"LI",{});var be=s(S);_=n(be,"A",{href:!0,target:!0,rel:!0,class:!0});var ye=s(_);ee=d(ye,"Dima presents at KITP 2023 (45 min)"),ye.forEach(r),be.forEach(r),l.forEach(r),c.forEach(r),this.h()},h(){t(E,"class","text-gray-500 text-sm italic my-8"),t(A,"class","abstract"),t(I,"class","svelte-1jp7cc4"),t(u,"href","https://arxiv.org/abs/2302.07253"),t(u,"target","_blank"),t(u,"rel","noreferrer"),t(u,"class","svelte-1jp7cc4"),t(m,"href","https://neurips.cc/virtual/2023/poster/71901"),t(m,"target","_blank"),t(m,"rel","noreferrer"),t(m,"class","svelte-1jp7cc4"),t(g,"href","https://github.com/bhoov/energy-transformer-jax"),t(g,"target","_blank"),t(g,"rel","noreferrer"),t(g,"class","svelte-1jp7cc4"),t(v,"href","https://github.com/Lemon-cmd/energy-transformer-torch"),t(v,"target","_blank"),t(v,"rel","noreferrer"),t(v,"class","svelte-1jp7cc4"),t(b,"href","https://recorder-v3.slideslive.com/#/share?share=88188&s=6a61b5bf-c59b-4ef5-b71f-be4156c92dd5"),t(b,"target","_blank"),t(b,"rel","noreferrer"),t(b,"class","svelte-1jp7cc4"),t(y,"href","https://twitter.com/Ben_Hoov/status/1730570603210404236"),t(y,"target","_blank"),t(y,"rel","noreferrer"),t(y,"class","svelte-1jp7cc4"),t(_,"href","https://online.kitp.ucsb.edu/online/deeplearning23/krotov/rm/jwvideo.html"),t(_,"target","_blank"),t(_,"rel","noreferrer"),t(_,"class","svelte-1jp7cc4")},m(H,c){we(H,o,c),e(o,E),e(E,k),e(o,w),e(o,T),e(T,D),e(o,W),e(o,A),e(A,X),e(o,B),e(o,I),e(I,J),e(o,z),e(o,i),e(i,P),e(P,u),e(u,K),e(i,O),e(i,x),e(x,m),e(m,C),e(i,F),e(i,L),e(L,g),e(g,U),e(i,G),e(i,j),e(j,v),e(v,Q),e(i,R),e(i,N),e(N,b),e(b,V),e(i,Y),e(i,q),e(q,y),e(y,Z),e(i,$),e(i,S),e(S,_),e(_,ee)},p:te,i:te,o:te,d(H){H&&r(o)}}}function Ie(M,o,E){let{data:k}=o;return M.$$set=w=>{"data"in w&&E(0,k=w.data)},[k]}class Pe extends _e{constructor(o){super(),Ee(this,o,Ie,Ae,ke,{data:0})}}export{Pe as component};
