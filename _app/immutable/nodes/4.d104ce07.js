import{S as le,i as ce,s as he,k as n,q as m,a as b,l as s,m as i,r as g,h as r,c as y,n as t,b as de,F as e,J as G}from"../chunks/index.6a9e29a0.js";function fe(q){let a,v,_,k,A,S,M,E,D,H,w,W,X,o,I,h,J,j,P,d,z,B,T,f,K,O,x,p,C,F,L,u,U;return{c(){a=n("main"),v=n("p"),_=m("WIP: barebones page to important links to this work"),k=b(),A=n("h1"),S=m("Energy Transformer"),M=b(),E=n("p"),D=m(`Our work combines aspects of three promising paradigms in machine learning,
    namely, attention mechanism, energy-based models, and associative memory.
    Attention is the power-house driving modern deep learning successes, but it
    lacks clear theoretical foundations. Energy-based models allow a principled
    approach to discriminative and generative tasks, but the design of the
    energy functional is not straightforward. At the same time, Dense
    Associative Memory models or Modern Hopfield Networks have a
    well-established theoretical foundation, and allow an intuitive design of
    the energy function. We propose a novel architecture, called the Energy
    Transformer (or ET for short), that uses a sequence of attention layers that
    are purposely designed to minimize a specifically engineered energy
    function, which is responsible for representing the relationships between
    the tokens. In this work, we introduce the theoretical foundations of ET,
    explore its empirical capabilities using the image completion task, and
    obtain strong quantitative results on the graph anomaly detection and graph
    classification tasks.`),H=b(),w=n("h2"),W=m("Links"),X=b(),o=n("ul"),I=n("li"),h=n("a"),J=m("Paper on ArXiv"),j=b(),P=n("li"),d=n("a"),z=m("NeurIPS 2023 virtual page"),B=b(),T=n("li"),f=n("a"),K=m("JAX code for Energy Transformer"),O=b(),x=n("li"),p=n("a"),C=m("Ben presents for NeurIPS 2023 (5 min)"),F=b(),L=n("li"),u=n("a"),U=m("Dima presents at KITP 2023 (45 min)"),this.h()},l(N){a=s(N,"MAIN",{});var l=i(a);v=s(l,"P",{class:!0});var Q=i(v);_=g(Q,"WIP: barebones page to important links to this work"),Q.forEach(r),k=y(l),A=s(l,"H1",{});var R=i(A);S=g(R,"Energy Transformer"),R.forEach(r),M=y(l),E=s(l,"P",{class:!0});var V=i(E);D=g(V,`Our work combines aspects of three promising paradigms in machine learning,
    namely, attention mechanism, energy-based models, and associative memory.
    Attention is the power-house driving modern deep learning successes, but it
    lacks clear theoretical foundations. Energy-based models allow a principled
    approach to discriminative and generative tasks, but the design of the
    energy functional is not straightforward. At the same time, Dense
    Associative Memory models or Modern Hopfield Networks have a
    well-established theoretical foundation, and allow an intuitive design of
    the energy function. We propose a novel architecture, called the Energy
    Transformer (or ET for short), that uses a sequence of attention layers that
    are purposely designed to minimize a specifically engineered energy
    function, which is responsible for representing the relationships between
    the tokens. In this work, we introduce the theoretical foundations of ET,
    explore its empirical capabilities using the image completion task, and
    obtain strong quantitative results on the graph anomaly detection and graph
    classification tasks.`),V.forEach(r),H=y(l),w=s(l,"H2",{class:!0});var Y=i(w);W=g(Y,"Links"),Y.forEach(r),X=y(l),o=s(l,"UL",{});var c=i(o);I=s(c,"LI",{});var Z=i(I);h=s(Z,"A",{href:!0,target:!0,rel:!0,class:!0});var $=i(h);J=g($,"Paper on ArXiv"),$.forEach(r),Z.forEach(r),j=y(c),P=s(c,"LI",{});var ee=i(P);d=s(ee,"A",{href:!0,target:!0,rel:!0,class:!0});var te=i(d);z=g(te,"NeurIPS 2023 virtual page"),te.forEach(r),ee.forEach(r),B=y(c),T=s(c,"LI",{});var ae=i(T);f=s(ae,"A",{href:!0,target:!0,rel:!0,class:!0});var re=i(f);K=g(re,"JAX code for Energy Transformer"),re.forEach(r),ae.forEach(r),O=y(c),x=s(c,"LI",{});var ne=i(x);p=s(ne,"A",{href:!0,target:!0,rel:!0,class:!0});var se=i(p);C=g(se,"Ben presents for NeurIPS 2023 (5 min)"),se.forEach(r),ne.forEach(r),F=y(c),L=s(c,"LI",{});var ie=i(L);u=s(ie,"A",{href:!0,target:!0,rel:!0,class:!0});var oe=i(u);U=g(oe,"Dima presents at KITP 2023 (45 min)"),oe.forEach(r),ie.forEach(r),c.forEach(r),l.forEach(r),this.h()},h(){t(v,"class","text-gray-500 text-sm italic my-8"),t(E,"class","abstract"),t(w,"class","svelte-1ll056c"),t(h,"href","https://arxiv.org/abs/2302.07253"),t(h,"target","_blank"),t(h,"rel","noreferrer"),t(h,"class","svelte-1ll056c"),t(d,"href","https://neurips.cc/virtual/2023/poster/71901"),t(d,"target","_blank"),t(d,"rel","noreferrer"),t(d,"class","svelte-1ll056c"),t(f,"href","https://github.com/bhoov/energy-transformer-jax"),t(f,"target","_blank"),t(f,"rel","noreferrer"),t(f,"class","svelte-1ll056c"),t(p,"href","https://recorder-v3.slideslive.com/#/share?share=88188&s=6a61b5bf-c59b-4ef5-b71f-be4156c92dd5"),t(p,"target","_blank"),t(p,"rel","noreferrer"),t(p,"class","svelte-1ll056c"),t(u,"href","https://online.kitp.ucsb.edu/online/deeplearning23/krotov/rm/jwvideo.html"),t(u,"target","_blank"),t(u,"rel","noreferrer"),t(u,"class","svelte-1ll056c")},m(N,l){de(N,a,l),e(a,v),e(v,_),e(a,k),e(a,A),e(A,S),e(a,M),e(a,E),e(E,D),e(a,H),e(a,w),e(w,W),e(a,X),e(a,o),e(o,I),e(I,h),e(h,J),e(o,j),e(o,P),e(P,d),e(d,z),e(o,B),e(o,T),e(T,f),e(f,K),e(o,O),e(o,x),e(x,p),e(p,C),e(o,F),e(o,L),e(L,u),e(u,U)},p:G,i:G,o:G,d(N){N&&r(a)}}}function pe(q,a,v){let{data:_}=a;return q.$$set=k=>{"data"in k&&v(0,_=k.data)},[_]}class me extends le{constructor(a){super(),ce(this,a,pe,fe,he,{data:0})}}export{me as component};
